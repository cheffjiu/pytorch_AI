{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入依赖的包\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义人造数据集函数\n",
    "def SyntheticData(w,b,num_examples):  #@save\n",
    "    #Generate y=Xw+b+noise\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) #X是样本张量，形状是样本数*样本特征，w是权重向量\n",
    "    y = torch.matmul(X, w) + b #y是预测值张量，形状是样本数*1\n",
    "    y+=torch.normal(0,0.01,y.shape) #加入噪声\n",
    "    return X, y.reshape((-1, 1)) #返回X和y,确保y是与X同形的二维张量\n",
    "\n",
    "#定义真实的权重weight和偏置bias\n",
    "ture_w = torch.tensor([2,-3.4])\n",
    "ture_b = 4.2\n",
    "#合成人造数据集  features：X是生成的样本, labels：y是由样本生产的预测值\n",
    "features,labels=SyntheticData(ture_w,ture_b,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features的每一行都是一个有2个特征二维的向量，labels的每一行都是一个有1个标签一维的标量\n",
    "print('features:', features[0], '\\n','label:', labels[0])\n",
    "\n",
    "#分别画出样本的第一个特征与预测值的散点图，能看出具有明显线性关系\n",
    "fig,axis=plt.subplots(1,2,figsize=(12,4))\n",
    "axis[0].scatter(features[:,0], labels[:,0],label='Scatter 1')\n",
    "axis[0].set_title('Scatter Plot of First Feature and Label')\n",
    "axis[0].legend()\n",
    "axis[1].scatter(features[:,1], labels[:,0],color=\"red\",label='Scatter 2')\n",
    "axis[1].set_title('Scatter Plot of Second Feature and Label')\n",
    "axis[1].legend()\n",
    "plt.show()\n",
    "plt.scatter(features[:,0], labels[:,0],label='Scatter 1')\n",
    "plt.scatter(features[:,1], labels[:,0],color=\"red\",label='Scatter 2')\n",
    "plt.legend()\n",
    "plt.title('Scatter Plot of First and Second Features and Label')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "def DataIter(batch_size,features,labels):\n",
    "    num_examples=features.shape[0]\n",
    "    indices=list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0,num_examples,batch_size):\n",
    "        batch_indices=torch.tensor(indices[i:min(i+batch_size,num_examples)])\n",
    "        yield features[batch_indices],labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化模型参数\n",
    "w=torch.normal(0,0.01,size=(2,1),requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "def LinearRegression(X,w,b):\n",
    "    return torch.matmul(X,w)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "def SquaredLoss(y_hat,y):\n",
    "    return (y_hat-y.reshape(-1,1))**2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化算法\n",
    "def sgd(params,lr,batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000049\n",
      "epoch 2, loss 0.000048\n",
      "epoch 3, loss 0.000048\n",
      "epoch 4, loss 0.000048\n",
      "epoch 5, loss 0.000048\n",
      "epoch 6, loss 0.000048\n",
      "epoch 7, loss 0.000048\n",
      "epoch 8, loss 0.000048\n",
      "epoch 9, loss 0.000048\n",
      "epoch 10, loss 0.000048\n",
      "epoch 11, loss 0.000048\n",
      "epoch 12, loss 0.000048\n",
      "epoch 13, loss 0.000048\n",
      "epoch 14, loss 0.000048\n",
      "epoch 15, loss 0.000048\n",
      "epoch 16, loss 0.000048\n",
      "epoch 17, loss 0.000048\n",
      "epoch 18, loss 0.000048\n",
      "epoch 19, loss 0.000048\n",
      "epoch 20, loss 0.000048\n",
      "epoch 21, loss 0.000048\n",
      "epoch 22, loss 0.000048\n",
      "epoch 23, loss 0.000048\n",
      "epoch 24, loss 0.000048\n",
      "epoch 25, loss 0.000048\n",
      "epoch 26, loss 0.000048\n",
      "epoch 27, loss 0.000048\n",
      "epoch 28, loss 0.000048\n",
      "epoch 29, loss 0.000048\n",
      "epoch 30, loss 0.000048\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "lr=0.01\n",
    "num_epochs=30\n",
    "batch_size=5\n",
    "net=LinearRegression\n",
    "loss=SquaredLoss\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in DataIter(batch_size,features,labels):\n",
    "        l=loss(net(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with torch.no_grad():\n",
    "        train_l=loss(net(features,w,b),labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
