{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.2144, Train Acc: 32.06%, Test Acc: 40.95%\n",
      "Epoch [2/50], Loss: 2.0549, Train Acc: 48.35%, Test Acc: 56.03%\n",
      "Epoch [3/50], Loss: 1.9358, Train Acc: 63.09%, Test Acc: 65.93%\n",
      "Epoch [4/50], Loss: 1.8773, Train Acc: 66.65%, Test Acc: 67.08%\n",
      "Epoch [5/50], Loss: 1.8432, Train Acc: 68.72%, Test Acc: 70.18%\n",
      "Epoch [6/50], Loss: 1.8159, Train Acc: 71.21%, Test Acc: 71.44%\n",
      "Epoch [7/50], Loss: 1.7992, Train Acc: 72.22%, Test Acc: 72.32%\n",
      "Epoch [8/50], Loss: 1.7869, Train Acc: 72.92%, Test Acc: 72.88%\n",
      "Epoch [9/50], Loss: 1.7771, Train Acc: 73.43%, Test Acc: 73.56%\n",
      "Epoch [10/50], Loss: 1.7693, Train Acc: 73.83%, Test Acc: 73.88%\n",
      "Epoch [11/50], Loss: 1.7631, Train Acc: 74.21%, Test Acc: 74.20%\n",
      "Epoch [12/50], Loss: 1.7576, Train Acc: 74.55%, Test Acc: 74.46%\n",
      "Epoch [13/50], Loss: 1.7529, Train Acc: 74.82%, Test Acc: 74.75%\n",
      "Epoch [14/50], Loss: 1.7488, Train Acc: 75.08%, Test Acc: 74.91%\n",
      "Epoch [15/50], Loss: 1.7450, Train Acc: 75.31%, Test Acc: 75.10%\n",
      "Epoch [16/50], Loss: 1.7414, Train Acc: 75.51%, Test Acc: 75.25%\n",
      "Epoch [17/50], Loss: 1.7383, Train Acc: 75.72%, Test Acc: 75.42%\n",
      "Epoch [18/50], Loss: 1.7352, Train Acc: 75.97%, Test Acc: 75.81%\n",
      "Epoch [19/50], Loss: 1.7321, Train Acc: 76.49%, Test Acc: 76.29%\n",
      "Epoch [20/50], Loss: 1.7291, Train Acc: 77.05%, Test Acc: 76.73%\n",
      "Epoch [21/50], Loss: 1.7260, Train Acc: 77.38%, Test Acc: 76.94%\n",
      "Epoch [22/50], Loss: 1.7233, Train Acc: 77.62%, Test Acc: 77.24%\n",
      "Epoch [23/50], Loss: 1.7204, Train Acc: 77.83%, Test Acc: 77.37%\n",
      "Epoch [24/50], Loss: 1.7177, Train Acc: 78.12%, Test Acc: 77.70%\n",
      "Epoch [25/50], Loss: 1.7149, Train Acc: 78.41%, Test Acc: 77.96%\n",
      "Epoch [26/50], Loss: 1.7122, Train Acc: 78.73%, Test Acc: 78.08%\n",
      "Epoch [27/50], Loss: 1.7099, Train Acc: 78.95%, Test Acc: 78.37%\n",
      "Epoch [28/50], Loss: 1.7076, Train Acc: 79.17%, Test Acc: 78.58%\n",
      "Epoch [29/50], Loss: 1.7055, Train Acc: 79.32%, Test Acc: 78.69%\n",
      "Epoch [30/50], Loss: 1.7034, Train Acc: 79.52%, Test Acc: 78.86%\n",
      "Epoch [31/50], Loss: 1.7016, Train Acc: 79.67%, Test Acc: 79.00%\n",
      "Epoch [32/50], Loss: 1.6999, Train Acc: 79.79%, Test Acc: 79.03%\n",
      "Epoch [33/50], Loss: 1.6983, Train Acc: 79.92%, Test Acc: 79.14%\n",
      "Epoch [34/50], Loss: 1.6966, Train Acc: 80.02%, Test Acc: 79.35%\n",
      "Epoch [35/50], Loss: 1.6951, Train Acc: 80.11%, Test Acc: 79.35%\n",
      "Epoch [36/50], Loss: 1.6937, Train Acc: 80.20%, Test Acc: 79.45%\n",
      "Epoch [37/50], Loss: 1.6924, Train Acc: 80.34%, Test Acc: 79.54%\n",
      "Epoch [38/50], Loss: 1.6913, Train Acc: 80.47%, Test Acc: 79.58%\n",
      "Epoch [39/50], Loss: 1.6898, Train Acc: 80.52%, Test Acc: 79.69%\n",
      "Epoch [40/50], Loss: 1.6888, Train Acc: 80.62%, Test Acc: 79.74%\n",
      "Epoch [41/50], Loss: 1.6877, Train Acc: 80.66%, Test Acc: 79.84%\n",
      "Epoch [42/50], Loss: 1.6864, Train Acc: 80.78%, Test Acc: 79.94%\n",
      "Epoch [43/50], Loss: 1.6855, Train Acc: 80.82%, Test Acc: 79.99%\n",
      "Epoch [44/50], Loss: 1.6847, Train Acc: 80.93%, Test Acc: 80.02%\n",
      "Epoch [45/50], Loss: 1.6836, Train Acc: 81.00%, Test Acc: 80.04%\n",
      "Epoch [46/50], Loss: 1.6826, Train Acc: 81.09%, Test Acc: 80.06%\n",
      "Epoch [47/50], Loss: 1.6817, Train Acc: 81.13%, Test Acc: 80.14%\n",
      "Epoch [48/50], Loss: 1.6809, Train Acc: 81.17%, Test Acc: 80.22%\n",
      "Epoch [49/50], Loss: 1.6800, Train Acc: 81.28%, Test Acc: 80.28%\n",
      "Epoch [50/50], Loss: 1.6792, Train Acc: 81.30%, Test Acc: 80.32%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "batch_size_ = 256\n",
    "num_epochs_ = 50\n",
    "learning_rate_ = 0.005\n",
    "transforms_ = v2.Compose(\n",
    "    [\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(\n",
    "            mean=[\n",
    "                0.5,\n",
    "            ],\n",
    "            std=[\n",
    "                0.5,\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "FashionMNIST数据集,包含10个类别的图像,每个类别有6000张训练图像和1000张测试图像\n",
    "    训练集:60,000张图像\n",
    "    测试集:10,000张图像\n",
    "    训练集和测试集的图像大小为28x28像素,每个像素点是一个灰度值,范围在0到1之间\n",
    "    训练集和测试集的图像标签为数字,范围在0到9之间\n",
    "\"\"\"\n",
    "train_dataset_ = datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, download=True, transform=transforms_\n",
    ")\n",
    "\n",
    "test_dataset_ = datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, download=True, transform=transforms_\n",
    ")\n",
    "\n",
    "\n",
    "train_loader_ = DataLoader(\n",
    "    dataset=train_dataset_, shuffle=True, num_workers=4, batch_size=batch_size_\n",
    ")\n",
    "test_loader_ = DataLoader(\n",
    "    dataset=test_dataset_, shuffle=True, num_workers=4, batch_size=batch_size_\n",
    ")\n",
    "\n",
    "\n",
    "class SoftmaxRegresion(nn.Module):\n",
    "    def __init__(self, input_dim=28 * 28, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        logits = self.linear(x)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "model_ = SoftmaxRegresion()\n",
    "criterion_ = nn.CrossEntropyLoss()\n",
    "optimizer_ = optim.SGD(model_.parameters(), lr=learning_rate_)\n",
    "\n",
    "writer = SummaryWriter(\"../logdir/softmax_regression\")\n",
    "dummy_input_ = torch.randn(1, 1, 28, 28)\n",
    "writer.add_graph(model_, dummy_input_)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_.to(device)\n",
    "tran_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs_):\n",
    "    model_.train()\n",
    "    total_loss = 0.0\n",
    "    correct_ = 0\n",
    "    total_ = 0\n",
    "    for i, (images, labels) in enumerate(train_loader_):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_(images)\n",
    "        loss = criterion_(outputs, labels)\n",
    "        optimizer_.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_ += labels.size(0)\n",
    "        correct_ += (predicted == labels).sum().item()\n",
    "        if i % 100 == 0:\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            writer.add_image(\n",
    "                \"trainning_images\", img_grid, global_step=epoch * len(train_loader_) + i\n",
    "            )\n",
    "    avg_loss = total_loss / len(train_loader_)\n",
    "    train_acc_ = 100 * correct_ / total_\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", train_acc_, epoch)\n",
    "\n",
    "    model_.eval()\n",
    "    test_correct_ = 0\n",
    "    test_total_ = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader_:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model_(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total_ += labels.size(0)\n",
    "            test_correct_ += (predicted == labels).sum().item()\n",
    "    test_acc_ = 100 * test_correct_ / test_total_\n",
    "    writer.add_scalar(\"Accuracy/test\", test_acc_, epoch)\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs_}], Loss: {avg_loss:.4f}, Train Acc: {train_acc_:.2f}%, Test Acc: {test_acc_:.2f}%\"\n",
    "    )\n",
    "    # 添加分类标签名称\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "writer.add_text('Class Labels', str(class_names))\n",
    "# 添加权重分布直方图\n",
    "for name, param in model_.named_parameters():\n",
    "    writer.add_histogram(name, param, epoch)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
